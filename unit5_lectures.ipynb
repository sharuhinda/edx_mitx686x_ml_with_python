{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('numpy version:', np.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Example of Value Iteration (Software Implementation)  \n",
    "\n",
    "Consider the grid with reward:  \n",
    "<img src=\"images/unit5_value_update_algo.png\" />    \n",
    "However, consider the following change to the transition probabilities: At any given grid location the agent can choose to either stay at the location or move to an adjacent grid location. If the agent chooses to stay at the location, such an action is successful with probability 1/2 and\n",
    "- if the agent is at the leftmost or rightmost grid location it ends up at its neighboring grid location with probability 1/2,\n",
    "- if the agent is at any of the inner grid locations it has a probability 1/4 each of ending up at either of the neighboring locations.\n",
    "  \n",
    "If the agent chooses to move (either left or right) at any of the inner grid locations, such an action is successful with probability 1/3 and with probability 2/3 it fails to move, and\n",
    "- if the agent chooses to move left at the leftmost grid location, then the action ends up exactly the same as choosing to stay, i.e., staying at the leftmost grid location with probability 1/2, and ends up at its neighboring grid location with probability 1/2,\n",
    "- if the agent chooses to move right at the rightmost grid location, then the action ends up exactly the same as choosing to stay, i.e., staying at the rightmost grid location with probability 1/2, and ends up at its neighboring grid location with probability 1/2.\n",
    "  \n",
    "[!!!] Note in this setting, we assume that the game does not halt after reaching the rightmost cell.\n",
    "  \n",
    "Let $\\gamma = 0.5$.  \n",
    "  \n",
    "Run the value iteration algorithm for 100 iterations. Use any computational software of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = ['s1', 's2', 's3', 's4', 's5']\n",
    "A = ['stay', 'left', 'right']\n",
    "T = np.array(\n",
    "    [\n",
    "        [[.5, .5, 0., 0., 0.], # a = 'stay', rows = s, columns = s'\n",
    "         [.25, .5, .25, 0., 0.],\n",
    "         [0., .25, .5, .25, 0.],\n",
    "         [0., 0., .25, .5, .25],\n",
    "         [0., 0., 0., .5, .5]],\n",
    "        [[.5, .5, 0., 0., 0.], # a = 'left', rows = s, columns = s'\n",
    "         [1/3, 2/3, 0., 0., 0.],\n",
    "         [0., 1/3, 2/3, 0., 0.],\n",
    "         [0., 0., 1/3, 2/3, 0.],\n",
    "         [0., 0., 0., 1/3, 2/3]],\n",
    "        [[2/3, 1/3, 0., 0., 0.], # a = 'right', rows = s, columns = s'\n",
    "         [0., 2/3, 1/3, 0., 0.],\n",
    "         [0., 0., 2/3, 1/3, 0.],\n",
    "         [0., 0., 0., 2/3, 1/3],\n",
    "         [0., 0., 0., .5, .5]]\n",
    "    ]\n",
    ")\n",
    "R = np.array([0., 0., 0., 0., 1.])\n",
    "V = np.zeros((5,))\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! R relates to current_state while V relates to future_state\n",
    "reward = R.reshape(-1, 1) + gamma * V.reshape(1, -1)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.5       ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33333333, 0.66666667]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.5       ]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T * reward).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T * reward).sum(axis=2).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_v(V, T, R):\n",
    "    \"\"\"\n",
    "    Returns updated value array (V)\n",
    "\n",
    "    \"\"\"\n",
    "    reward = R.reshape(-1, 1) + gamma * V.reshape(1, -1) # computing reward part (R(s) + gamma * V_k(s')) !!! it's IMPORTANT that s' !!!\n",
    "    reward_lh = T * reward # computing reward weighted by likelihood (probabilities) for each current_state, future_state and action => shape (3, 5, 5)\n",
    "    Q = reward_lh.sum(axis=2) # summing up across last dimension (future_states) => shape (3, 5) actions x current_states\n",
    "    V_new = Q.max(axis=0) # determining best variant for each state across possible actions => shape (5,)\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_upd = update_v(V=V, T=T, R=R)\n",
    "V_upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.16666667, 1.33333333])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_upd = update_v(V=V_upd, T=T, R=R)\n",
    "V_upd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems working. Let's use loop to end up with solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00833333, 0.025     , 0.1       , 0.4       , 1.6       ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    V = update_v(V=V, T=T, R=R)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00833333, 0.025     , 0.1       , 0.4       , 1.6       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate 100 cycles more to compare results\n",
    "for _ in range(100):\n",
    "    V = update_v(V=V, T=T, R=R)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00761459, 0.02415681, 0.09904883, 0.39902345, 1.59902343])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.zeros((5,))\n",
    "for _ in range(10):\n",
    "    V = update_v(V=V, T=T, R=R)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00761459 0.02415681 0.09904883 0.39902345 1.59902343]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "state = [0, 1, 2, 3, 4]\n",
    "action = [0, 1, 2] #, representing moving left, staying, moving right respectively \n",
    "#transition probability\n",
    "T = np.array(\n",
    "    [\n",
    "        [[1/2,1/2,0,0,0],\n",
    "         [1/2,1/2,0,0,0],\n",
    "         [2/3,1/3,0,0,0]],\n",
    "        [[1/3,2/3,0,0,0],\n",
    "         [1/4,1/2,1/4,0,0],\n",
    "         [0,2/3,1/3,0,0]],\n",
    "        [[0,1/3,2/3,0,0],\n",
    "         [0,1/4,1/2,1/4,0],\n",
    "         [0,0,2/3,1/3,0]],\n",
    "        [[0,0,1/3,2/3,0],\n",
    "         [0,0,1/4,1/2,1/4],\n",
    "         [0,0,0,2/3,1/3]],\n",
    "        [[0,0,0,1/3,2/3],\n",
    "         [0,0,0,1/2,1/2],\n",
    "         [0,0,0,1/2,1/2]]\n",
    "    ])\n",
    "num_state = 5\n",
    "num_action = 3\n",
    "r = 1/2\n",
    "# initialization\n",
    "V = np.zeros(5)\n",
    "# reward\n",
    "R = np.zeros(5)\n",
    "R[4] = 1\n",
    "num_iter = 10\n",
    "for i in range(num_iter):\n",
    "    Q = [[sum([T[s][a][t] * (R[s] + r * V[t]) for t in range(num_state)]) for a in range(num_action)] for s in range(num_state)]\n",
    "    V = np.max(Q, axis=1)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
