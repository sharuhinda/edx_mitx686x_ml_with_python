{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.5\n",
      "matplotlib version: 3.6.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('numpy version:', np.__version__)\n",
    "\n",
    "import matplotlib as mpl\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will analyze a simple neural network to understand its classification properties. Consider the neural network given in the figure below, with **ReLU activation functions (denoted by $f$) on all neurons, and a softmax activation function in the output layer**:  \n",
    "<img src=\"images/homework3_nn.png\" alt=\"network architecture\" width=\"400\" height=\"250\">  \n",
    "Given an input $x=[x_1, x_2]^T$, the hidden units in the network are activated in stages as described by the following equations:\n",
    "$$\n",
    "z_i = x_1W_{1i} + x_2W_{2i} + W_{0i}, \\  f(z_i) = \\max\\{z_i, 0\\}, \\  i=1..4  \\\\\n",
    "u_j = \\sum_i{f(z_i)V_{ij}}+V_{0j}, \\  f(u_j) = \\max\\{u_j, 0\\}, \\  j=1..2\n",
    "$$\n",
    "The final output of the network is obtained by applying the **softmax** function to the last hidden layer,  \n",
    "$$\n",
    "o_j = \\frac{e^{f(u_j)}}{\\sum_k{e^{f(u_k)}}}, \\  j=1..2\n",
    "$$ \t\t\t \t \n",
    "In this problem, we will consider the following setting of parameters:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "W_{11} & W_{21} & W_{01} \\\\ W_{12} & W_{22} & W_{02} \\\\\n",
    "W_{13} & W_{23} & W_{03} \\\\ W_{14} & W_{24} & W_{04}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\ 0 & 1 & -1 \\\\\n",
    "-1 & 0 & -1 \\\\ 0 & -1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "V_{11} & V_{21} & V_{31} & V_{41} & V_{01} \\\\\n",
    "V_{12} & V_{22} & V_{32} & V_{42} & V_{02}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 0 \\\\\n",
    "-1 & -1 & -1 & -1 & 2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0, -1],\n",
       "       [ 0,  1, -1],\n",
       "       [-1,  0, -1],\n",
       "       [ 0, -1, -1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([1, 0, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1]).reshape(4, -1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  0],\n",
       "       [-1, -1, -1, -1,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([1, 1, 1, 1, 0, -1, -1, -1, -1, 2]).reshape(2, -1)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.max([np.zeros_like(x), x], axis=0)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2\n"
     ]
    }
   ],
   "source": [
    "print(relu(-3), relu(0), relu(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00242826 0.01794253 0.97962921] 1.0\n"
     ]
    }
   ],
   "source": [
    "tmp = softmax(np.array([-2, 0, 4]))\n",
    "print(tmp, tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [14],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3, 14])\n",
    "np.concatenate([x, [1]]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2],\n",
       "       [ 13],\n",
       "       [ -4],\n",
       "       [-15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ np.concatenate([x, [1]]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [13],\n",
       "       [ 0],\n",
       "       [ 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_output = relu(W @ np.concatenate([x, [1]]).reshape(-1, 1))\n",
    "hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.],\n",
       "       [13.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_in = np.concatenate([hidden_output, np.ones((1, hidden_output.shape[1]))], axis=0)\n",
    "output_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15.],\n",
       "       [-13.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V @ output_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(V @ output_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999694e-01],\n",
       "       [3.05902227e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(relu(V @ output_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11920292, 0.88079708])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([0, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95257413, 0.04742587])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([3, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3022515928828513"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(999) / 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows a single LSTM unit that consists of Input, Output, and Forget gates.  \n",
    "\\![LSTM unit scheme](images/homework3_lstm_scheme.png)  \n",
    "<img src=\"images/homework3_lstm_scheme.png\" alt=\"lstm unit scheme\" width=\"300\" height=\"250\">  \n",
    "The behavior of such a unit as a recurrent neural network is specified by a set of update equations. These equations define how the gates, “memory cell\" $c_t$ and the “visible state\" $h_t$ are updated in response to input $x_t$ and previous states $c_{t-1}$, $h_{t-1}$.  \n",
    "For the LSTM unit,\n",
    "$$\n",
    "f_t = \\text{sigmoid}(W^{f, h}h_{t-1} + W^{f, x}x_t + b_f) \\\\\n",
    "i_t = \\text{sigmoid}(W^{i, h}h_{t-1} + W^{i, x}x_t + b_i) \\\\\n",
    "o_t = \\text{sigmoid}(W^{o, h}h_{t-1} + W^{o, x}x_t + b_o) \\\\\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tanh(W^{c,h}h_{t-1} + W^{c,x}x_t + b_c) \\\\\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "where $\\odot$ stands for element-wise multiplication. The adjustable parameters in this unit are matrices W as well as offset parameter vectors b. By changing these parameters, we change how the unit evolves as a function of inputs $x_t$  \n",
    "To keep things simple, in this problem we assume that $x_t$, $c_t$ and $h_t$ are all scalars. Concretely, suppose that the parameters are given by  \n",
    "$$\n",
    "\\begin{matrix}\n",
    "W^{f,h}=0 & W^{f, x}=0 & b_f=-100 & W^{c,h}=-100 \\\\\n",
    "W^{i,h}=0 & W^{i,x}=100 & b_i=100 & W^{c,x}=50 \\\\\n",
    "W^{o,h}=0 & W^{o,x}=100 & b_o=0 & b_c=0\n",
    "\\end{matrix}\n",
    "$$\n",
    "We run this unit with initial conditions $h_{-1}=0$ and $c_{-1}=0$, and in response to the following input sequence: [0, 0, 1, 1, 1, 0] (For example, $x_0=0$, $x_1=0$, $x_2=1$ and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sequence h_0, h_1, ... h_5 (round h_i to closest integer on every step)\n",
    "def calc_f_t(h, x):\n",
    "    lin = 0*h + 0*x - 100\n",
    "    return 0 # sigmoid(-100) -> 0\n",
    "\n",
    "def calc_i_t(h, x):\n",
    "    lin = 0*h + 100*x + 100\n",
    "    if lin <= -1:\n",
    "        return 0\n",
    "    elif lin >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-lin))\n",
    "    \n",
    "def calc_o_t(h, x):\n",
    "    lin = 0*h + 100*x\n",
    "    if lin <= -1:\n",
    "        return 0\n",
    "    elif lin >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-lin))\n",
    "    \n",
    "def calc_c_t(c, f, i, h, x):\n",
    "    lin = -100*h + 50*x\n",
    "    if lin <= -1:\n",
    "        lin = -1\n",
    "    elif lin >= 1:\n",
    "        lin = 1\n",
    "    else:\n",
    "        lin = np.tanh(lin)\n",
    "    return f*c + i*lin\n",
    "\n",
    "def calc_h_t(o, c):\n",
    "    tmp = np.tanh(c)\n",
    "    if tmp <= -1:\n",
    "        tmp = -1\n",
    "    elif tmp >= 1:\n",
    "        tmp = 1\n",
    "    return o * tmp\n",
    "\n",
    "def take_step(h, c, x):\n",
    "    f_t = calc_f_t(h, x)\n",
    "    i_t = calc_i_t(h, x)\n",
    "    o_t = calc_o_t(h, x)\n",
    "    c_t = calc_c_t(c, f_t, i_t, h, x)\n",
    "    h_t = calc_h_t(o_t, c_t)\n",
    "    print('Parameter values:')\n",
    "    print('f_t = ', f_t)\n",
    "    print('i_t = ', i_t)\n",
    "    print('o_t = ', o_t)\n",
    "    print('c_t = ', c_t)\n",
    "    print('h_t = ', h_t)\n",
    "    return (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  0.5\n",
      "c_t =  0.0\n",
      "h_t =  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating sequence for x = [0, 0, 1, 1, 1, 0]\n",
    "take_step(0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  0.5\n",
      "c_t =  0.0\n",
      "h_t =  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_step(0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  1\n",
      "h_t =  0.7615941559557649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7615941559557649, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_step(0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  -1\n",
      "h_t =  -0.7615941559557649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.7615941559557649, -1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_step(1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  1\n",
      "h_t =  0.7615941559557649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7615941559557649, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_step(-1, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  0.5\n",
      "c_t =  -1\n",
      "h_t =  -0.3807970779778824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.3807970779778824, -1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_step(1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_several_steps(h_0, c_0, X):\n",
    "    h = h_0\n",
    "    c = c_0\n",
    "    for i, x in enumerate(X):\n",
    "        print('STEP #', i+1)\n",
    "        h, c = take_step(h, c, x)\n",
    "        if h >= -0.5 and h <= 0.5:\n",
    "            h = 0\n",
    "        else:\n",
    "            h = np.round(h, 0)\n",
    "        print('Corrected value of h = ', h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP # 1\n",
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  1\n",
      "h_t =  0.7615941559557649\n",
      "Corrected value of h =  1.0\n",
      "STEP # 2\n",
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  -1\n",
      "h_t =  -0.7615941559557649\n",
      "Corrected value of h =  -1.0\n",
      "STEP # 3\n",
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  0.5\n",
      "c_t =  1\n",
      "h_t =  0.3807970779778824\n",
      "Corrected value of h =  0\n",
      "STEP # 4\n",
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  1\n",
      "h_t =  0.7615941559557649\n",
      "Corrected value of h =  1.0\n",
      "STEP # 5\n",
      "Parameter values:\n",
      "f_t =  0\n",
      "i_t =  1\n",
      "o_t =  1\n",
      "c_t =  -1\n",
      "h_t =  -0.7615941559557649\n",
      "Corrected value of h =  -1.0\n"
     ]
    }
   ],
   "source": [
    "X = [1, 1, 0, 1, 1]\n",
    "take_several_steps(0, 0, X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key steps for training multi-layer neural networks is stochastic gradient descent. We will use the back-propagation algorithm to compute the gradient of the loss function with respect to the model parameters.  \n",
    "Consider the L-layer neural network below:  \n",
    "<img src=\"images/homework3_nn_backprop.png\" alt=\"nn scheme\" width=\"400\" height=\"250\">  \n",
    "In the following problems, we will the following notation: $b_j^l$ is the bias of the j-th neuron in the l-th layer, $a_j^l$ is the activation of j-th neuron in the l-th layer, and $w_{jk}^l$ is the weight for the connection from the k-th neuron in the (l-1)-th layer to the j-th neuron in the l-th layer.  \n",
    "If the activation function is $f$ and the loss function we are minimizing is C, then the equations describing the network are:  \n",
    "$$\n",
    "a_j^l = f(\\sum_k{w_{jk}^l a_k^{l-1}} + b_j^l) \\\\\n",
    "\\text{Loss} = C(a^L)\n",
    "$$\n",
    "Note that notations without subscript denote the corresponding vector or matrix, so that $a^l$ is activation vector of the l-th layer, and $w^l$ is the weights matrix in l-th layer for $l=1,..,L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10499359 0.25       0.19661193 0.01766271]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([-2, 0, 1, 4])\n",
    "print(sigmoid(X)*(1-sigmoid(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24048908305088898"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28842841648243966"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sigmoid(-1.15) - 1)**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13872777081136367"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dC / db\n",
    "z_2 = -1.15\n",
    "(sigmoid(z_2)-1)*sigmoid(z_2)*(1-sigmoid(z_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00416183312434091"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dC / dw_2\n",
    "(sigmoid(z_2)-1)*sigmoid(z_2)*(1-sigmoid(z_2))*0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0809165621704553"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dC / dw_1\n",
    "(sigmoid(z_2)-1)*sigmoid(z_2)*(1-sigmoid(z_2))*(-5)*3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
