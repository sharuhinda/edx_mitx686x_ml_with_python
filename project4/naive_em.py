"""Mixture model using EM"""
from typing import Tuple
import numpy as np
from common import GaussianMixture

def get_point_likelihood(point: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
    """
    Returns the likelyhood of single observation
    """
    cols = np.nonzero(point)[0]
    d = cols.shape[0]
    power = - ((point[cols] - mixture.mu[:, cols])**2).sum(axis=1) / (2 * mixture.var)
    return mixture.p * (np.exp(power) / ((2 * np.pi * mixture.var)**(d/2)))


def get_likelihood(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
    n = X.shape[0]
    likelihood = None
    for i in range(n):
        if likelihood is None:
            likelihood = get_point_likelihood(point=X[i], mixture=mixture).reshape(1, -1)
        else:
            likelihood = np.r_[likelihood, get_point_likelihood(point=X[i], mixture=mixture).reshape(1, -1)]
    return likelihood

# alternative version (doesn't filter out 0-values in each X row)
def get_cluster_likelihood(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
    """
    Returns likelihoods of points being generated by each mixture component
    Assumes all points are valid (no missing / incorrect values)
    Result shape: [n, k]
    """
    n, d = X.shape
    K = mixture.p.shape[0]
    likelihood = None
    for k in range(K):
        # power of e in expression of likelihood of x being generated by cluster with parameters mu and var
        power = - ((X - mixture.mu[k])**2).sum(axis=1) / (2 * mixture.var[k])
        if likelihood is None:
            likelihood = mixture.p[k] * (np.exp(power) / ((2 * np.pi * mixture.var[k])**(d/2)))
        else:
            likelihood = np.c_[likelihood, mixture.p[k] * (np.exp(power) / ((2 * np.pi * mixture.var[k])**(d/2)))]
    return likelihood


def estep(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:
    """E-step: Softly assigns each datapoint to a gaussian component

    Args:
        X: (n, d) array holding the data
        mixture: the current gaussian mixture

    Returns:
        np.ndarray: (n, K) array holding the soft counts (posteriors)
            for all components for all examples
        float: log-likelihood of the assignment
    """
    n = X.shape[0]
    lh = None # initialize likelihood value
    for i in range(n):
        cols = np.nonzero(X[i])[0]
        d = cols.shape[0]
        power = - ((X[i, cols] - mixture.mu[:, cols])**2).sum(axis=1) / (2 * mixture.var)
        lh_point = mixture.p * (np.exp(power) / ((2 * np.pi * mixture.var)**(d/2)))
        if lh is None:
            lh = lh_point.reshape(1, -1).copy()
        else:
            lh = np.r_[lh, lh_point.reshape(1, -1).copy()]
    loglh = np.log(lh.sum(axis=1)).sum()
    lh /= lh.sum(axis=1).reshape(-1, 1)
    return (lh, loglh)
    raise NotImplementedError


def mstep(X: np.ndarray, post: np.ndarray) -> GaussianMixture:
    """M-step: Updates the gaussian mixture by maximizing the log-likelihood
    of the weighted dataset

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
    """
    n, d = X.shape
    K = post.shape[1]
    
    n_hat = post.sum(axis=0) # shape (K, )
    p = n_hat / n # shape (K, )
    mu = (post.T @ X) / n_hat.reshape(-1, 1) # (K, n) @ (n, d) => (K, d), then / (K, 1) => (K, d)
    
    var = np.zeros((K,))
    for i in range(n):
        cols = np.nonzero(X[i])[0]
        var += ((X[i, cols] - mu[:, cols])**2).sum(axis=1) * post[i] # (d, ) - (K, d) => (K, d), then sum(axis=1) => (K, ) * (K, )
    var /= (n_hat * d)

    return GaussianMixture(p=p, mu=mu, var=var)
    raise NotImplementedError


def run(X: np.ndarray, post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:
    """Runs the mixture model

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples (posterior probability, p(j|x^i))
        float: log-likelihood of the current assignment (log-likelihood of the weighted dataset)
    """
    epsilon = 1e-6
    old_ll = None
    new_ll = None
    while (old_ll is None) or (np.abs(old_ll-new_ll) > epsilon * np.abs(old_ll)):
        old_ll = new_ll
        posterior, new_ll = estep(X=X, mixture=mixture)
        mixture = mstep(X, post=posterior)

    return (mixture, post, new_ll)
    raise NotImplementedError


def estep_for_incomplete(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:
    n = X.shape[0]
    lh = None # initialize likelihood (posterior) value
    for i in range(n):
        cols = np.nonzero(X[i])[0]
        d = cols.shape[0]
        power = - ((X[i, cols] - mixture.mu[:, cols])**2).sum(axis=1) / (2 * mixture.var)
        lh_point = mixture.p * (np.exp(power) / ((2 * np.pi * mixture.var)**(d/2)))
        if lh is None:
            lh = lh_point.reshape(1, -1).copy()
        else:
            lh = np.r_[lh, lh_point.reshape(1, -1).copy()]
    loglh = np.log(lh.sum(axis=1)).sum()
    lh /= lh.sum(axis=1).reshape(-1, 1)
    return (lh, loglh)


def estep_for_incomplete_mod(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:
    
    def log_N_uj(x: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
        cols = np.nonzero(x)[0]
        d = cols.shape[0]
        power = - ((x[cols] - mixture.mu[:, cols])**2).sum(axis=1) / (2 * mixture.var)
        return  power - (d / 2) * np.log(2 * np.pi * mixture.var)
    
    def f_uj(x: np.ndarray, mixture: GaussianMixture) -> np.ndarray: # will return shape (k,)
        return np.log(mixture.p) + log_N_uj(x=x, mixture=mixture)
    
    n, d = X.shape
    log_posterior = None
    for u in range(n):
        # value of f(u, j)
        f = f_uj(x=X[u], mixture=mixture)
        if log_posterior is None:
            log_posterior = f.reshape(1, -1).copy()
        else:
            log_posterior = np.r_[log_posterior, f.reshape(1, -1).copy()]
    likelihood = np.log(np.exp(log_posterior).sum(axis=1)).sum()
    
    log_posterior -= np.log(np.exp(log_posterior).sum(axis=1)).reshape(-1, 1)
    return (np.exp(log_posterior), likelihood)


def mstep_for_incomplete(X: np.ndarray, post: np.ndarray, mixture: GaussianMixture,
          min_variance: float = .25) -> GaussianMixture:
    """M-step: Updates the gaussian mixture by maximizing the log-likelihood
    of the weighted dataset

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
    """
    n, d = X.shape
    K = post.shape[1]
    indicator = np.where(X > 0, 1, 0)
    n_hat = post.sum(axis=0) # shape (K, )
    p = n_hat / n # shape (K, )

    mu = np.zeros((K, d))
    for k in range(K):
        for coord in range(d):
            denom = post[:, k] * indicator[:, coord]
            if denom.sum() >= 1.:
                mu[k, coord] = (post[:, k] * indicator[:, coord] * X[:, coord]).sum() / denom.sum()
            else:
                mu[k, coord] = mixture.mu[k, coord]

    #mu = (post.T @ X) / n_hat.reshape(-1, 1) # (K, n) @ (n, d) => (K, d), then / (K, 1) => (K, d)
    
    var = np.zeros((K,))
    denom = np.zeros((K,))
    for i in range(n):
        cols = np.nonzero(X[i])[0]
        var += ((X[i, cols] - mu[:, cols])**2).sum(axis=1) * post[i] # (d, ) - (K, d) => (K, d), then sum(axis=1) => (K, ) * (K, )
        denom += post[i] * cols.shape[0]
    var /= denom
    var = np.maximum(var, np.ones_like(var)*min_variance)

    return GaussianMixture(p=p, mu=mu, var=var)
    raise NotImplementedError


def run_for_incomplete(X: np.ndarray, mixture: GaussianMixture,
        post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:
    """Runs the mixture model

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples (posterior probability, p(j|x^i))
        float: log-likelihood of the current assignment (log-likelihood of the weighted dataset)
    """
    epsilon = 1e-6
    old_loglh = None
    new_loglh = None
    while (old_loglh is None) or (np.abs(old_loglh-new_loglh) > epsilon * np.abs(old_loglh)):
        old_loglh = new_loglh
        posterior, new_loglh = estep_for_incomplete_mod(X=X, mixture=mixture)
        mixture = mstep_for_incomplete(X, post=posterior, mixture=mixture, min_variance=.25)

    return (mixture, post, new_loglh)
    raise NotImplementedError